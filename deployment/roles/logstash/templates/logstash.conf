# {{ ansible_managed }}

input {
  stdin {
    type => "stdin-type"
  }

  file {
    type => "syslog"

    # Wildcards work, here :)
    path => [ "/var/syslog/**/*.log" ]

    sincedb_path => "/var/lib/logstash/.sincedb"

    start_position => "beginning"
  }
}

filter {
  if [type] == "syslog" {
    grok {
      overwrite => "message"
      match => {
        "message" => "^%{TIMESTAMP_ISO8601:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} (?:%{PROG:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: )?%{GREEDYDATA:syslog_message}"
      }
    }
    syslog_pri { }
    date {
      match => [ "syslog_timestamp", "ISO8601" ]
      remove_field => [ "syslog_timestamp" ]
    }
    if !("_grokparsefailure" in [tags]) {
      mutate {
        replace => [ "@source_host", "%{syslog_hostname}" ]
        replace => [ "@message",     "%{syslog_message}" ]
      }

      mutate {
        remove_field => [ "syslog_hostname", "syslog_message", "syslog_timestamp", "message" ]
      }
    }
  }
  if [type] == "apache" {
    grok {
      # See the following URL for a complete list of named patterns
      # logstash/grok ships with by default:
      # https://github.com/logstash/logstash/tree/master/patterns
      #
      # The grok filter will use the below pattern and on successful match use
      # any captured values as new fields in the event.
      match => [ "message", "%{COMBINEDAPACHELOG} %{IPORHOST:vhost}" ]
    }
    date {
      # Try to pull the timestamp from the 'timestamp' field (parsed above with
      # grok). The apache time format looks like: "18/Aug/2011:05:44:34 -0700"
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    mutate {
      convert => [ "bytes", "integer" ]
      lowercase => [ "vhost" ]
    }
    geoip {
      source => "clientip"
    }
    useragent {
      source => "agent"
    }
  }
}

output {
  stdout { }
  elasticsearch {
    host => "127.0.0.1"
    port => "9200"
    protocol => "http"
    flush_size => 100
  }
}

